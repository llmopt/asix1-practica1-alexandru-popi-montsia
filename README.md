# ASIX_DAW_IA
Testeig de models d'IA i primeres aplicacions.

## Pregunta-li a una IA com utilitzar el model Mistral-7B-Instruct (o un equivalent) de hugging face per a interpretar logs del Apache web server, realitzant un programa amb python. Realitza el programa i testeja‚Äôl.

Abans d‚Äôexecutar cap prova assegurat de saber quants recursos hardware utilitzar√†, i valora si conv√© o no executar-lo. Si no conv√© executar-lo, tracta de buscar altra opci√≥. 

Si no es viable usar aquest model, intenteu usar-ne d‚Äôaltre.

Lliureu la conversa amb la IA, el programa python que heu creat i el test que heu realitzat.

# Explicaci√≥ de Paraules Claus
- RAG (Retrieval Augmented Generation): √âs una t√®cnica de processament del llenguatge natural (NLP) que combina els punts forts dels models d'intel¬∑lig√®ncia artificial basada en recuperaci√≥ i generativa (IA), aix√≥ permet a la IA recuperar informaci√≥ rellevant per millorar les seves respostes generades.

- Fine-Tuned: El proc√©s de prendre un model d'aprenentatge autom√†tic preentrenat i la formaci√≥ cont√≠nua en un conjunt de dades m√©s petit i dirigit.

- Temperature: Defineix la previsibilitat de la seva sortida. Les temperatures m√©s altes produeixen resultats m√©s creatius, mentre que les temperatures m√©s baixes produeixen respostes m√©s predictibles.

# Com executar l'API
- Crear un compte en [HuggingFace](https://huggingface.co/)

- Crear un token
![image](https://github.com/user-attachments/assets/86fff3a8-3312-490d-ae1f-dcb936b42d85)

- Inserim el token dins del script de Python

# Posada en marxa del fitxer "IA.py":
- Clonem el repositori:
  "git clone https://github.com/llmopt/asix1-practica1-alexandru-popi-montsia"

- Entrem dins del directori:
  "cd asix1-practica1-adamoughriss"

- Executem el script:
  "python3 IA.py"

# Resultat del test de l'IA
- Log proporcionat:
  "[Tue May 20 10:04:31.525155 2025] [mpm_event:notice] [pid 68661:tid 140321180847168] AH00489: Apache/2.4.41 (Ubuntu) configured -- resuming normal operations"
  "[Tue May 20 10:04:31.525799 2025] [core:notice] [pid 68661:tid 140321180847168] AH00094: Command line: '/usr/sbin/apache2'"

- Contestaci√≥ donada per part de l'IA:
  "The log you provided is a normal Apache startup log, indicating that the server (Apache/2.4.41, running on Ubuntu) has been configured and is resuming normal
  operations. There are no errors in this log. However, it's worth noting that the date mentioned in the log seems to be from the future (May 20, 2025), which may
  be a mistake or a placeholder.

  Normally, session errors, internal server errors, or other issues would be prefaced with the text [error] instead of [notice], as in the case of Apache/2.4.41
  (Ubuntu) AH00526: Syntax error on line 123 of /etc/apache2/apache2.conf:
  Invalid command 'Fnord', despite this log line being syntactically correct, it would still be an error if found in an actual configuration file."

# Difer√®ncies entre Descarregar el Model i utilitzar l'API
| Caracter√≠stica             | **Model en Client (Descarregat)**                      | **Importaci√≥ de l'API (Servei remot)**       |
| -------------------------- | ------------------------------------------------------ | -------------------------------------------- |
| **Acc√©s a Internet**       | No necessari un cop descarregat                        | Necessari per cada petici√≥                   |
| **Rendiment (lat√®ncia)**   | M√©s r√†pid, sense connexi√≥ externa                      | Pot ser m√©s lent per lat√®ncia de xarxa       |
| **Control sobre el model** | Total, es pot modificar, ajustar o entrenar            | Limitat al que ofereix el prove√Ødor          |
| **Actualitzacions**        | Manual (cal tornar a descarregar el model actualitzat) | Autom√†tiques, gestionades pel prove√Ødor      |
| **Cost**                   | Cost √∫nic, infraestructura pr√≤pia                      | Cost recurrent (per √∫s, subscripci√≥)         |
| **Facilitat d'integraci√≥** | Pot requerir m√©s configuraci√≥ t√®cnica                  | F√†cil, nom√©s cal fer crides a l'API          |
| **Privadesa de dades**     | M√†xima, les dades no surten del dispositiu             | Risc de privadesa si s‚Äôenvien dades al n√∫vol |
| **Escalabilitat**          | Limitada al maquinari local                            | Alta, gestionada pel prove√Ødor del servei    |




















# ü§ñ Exemple b√†sic d'√∫s de BlenderBot amb Hugging Face Transformers

Aquest exemple mostra com utilitzar el model [`facebook/blenderbot-400M-distill`](https://huggingface.co/facebook/blenderbot-400M-distill) per generar respostes a preguntes o missatges de l'usuari, utilitzant el `pipeline` de Hugging Face.

## üì¶ Requisits

Assegura't de tenir instal¬∑lades les depend√®ncies seg√ºents:

```bash
pip install transformers torch
```

## üìú Codi complet

```python
from transformers import pipeline

# Inicialitzem el pipeline amb BlenderBot
# https://huggingface.co/facebook/blenderbot-400M-distill
chatbot = pipeline(task="text2text-generation", model="facebook/blenderbot-400M-distill")

# Missatge de l'usuari
user_message = """
What are some fun activities I can do in the winter?
"""

# Enviem el missatge al model i generem resposta
output = chatbot(user_message)

# Mostrem la resposta generada
print("Bot:", output[0]["generated_text"])
```

## üîç Explicaci√≥ del funcionament

### 1. `chatbot = pipeline(...)`

Aquesta l√≠nia crea un objecte de tipus `Text2TextGenerationPipeline`, especialitzat en tasques de generaci√≥ de text a partir de text (`text2text-generation`).

El model que s'utilitza √©s `facebook/blenderbot-400M-distill`, un model preentrenat per mantenir converses.

### 2. Qu√® √©s `chatbot`

`chatbot` √©s un objecte que es pot cridar com una funci√≥ perqu√® implementa el m√®tode especial `__call__()`.

Gr√†cies a aix√≤, pots fer:

```python
output = chatbot(user_message)
```

### 3. Qu√® fa `chatbot(user_message)`?

- Tokenitza el text d'entrada.
- El passa pel model preentrenat.
- Des-tokenitza la sortida i la retorna en una llista de diccionaris, on cada diccionari cont√© la clau `"generated_text"` amb la resposta generada.

**Exemple de sortida:**

```python
[{'generated_text': 'You can go skiing, ice skating, or drink hot chocolate by the fire.'}]
```

### 4. `print("Bot:", output[0]["generated_text"])`

Accedeix a la primera (i √∫nica) resposta generada i imprimeix el contingut textual.

## ‚úÖ Resultat esperat

```
Bot: You can go skiing, ice skating, or drink hot chocolate by the fire.
```
