# ASIX_DAW_IA
Testeig de models d'IA i primeres aplicacions.


## Pregunta-li a una IA com utilitzar el model Mistral-7B-Instruct (o un equivalent) de hugging face per a interpretar logs del Apache web server, realitzant un programa amb python. Realitza el programa i testeja‚Äôl.

Abans d‚Äôexecutar cap prova assegurat de saber quants recursos hardware utilitzar√†, i valora si conv√© o no executar-lo. Si no conv√© executar-lo, tracta de buscar altra opci√≥. 

Si no es viable usar aquest model, intenteu usar-ne d‚Äôaltre.

Lliureu la conversa amb la IA, el programa python que heu creat i el test que heu realitzat.


# Explicaci√≥ de Paraules Claus
- RAG (Retrieval Augmented Generation): √âs una t√®cnica de processament del llenguatge natural (NLP) que combina els punts forts dels models d'intel¬∑lig√®ncia artificial basada en recuperaci√≥ i generativa (IA), aix√≥ permet a la IA recuperar informaci√≥ rellevant per millorar les seves respostes generades.

- Fine-Tuned: El proc√©s de prendre un model d'aprenentatge autom√†tic preentrenat i la formaci√≥ cont√≠nua en un conjunt de dades m√©s petit i dirigit.

- Temperature: Defineix la previsibilitat de la seva sortida. Les temperatures m√©s altes produeixen resultats m√©s creatius, mentre que les temperatures m√©s baixes produeixen respostes m√©s predictibles.



# Difer√®ncies entre Descarregar el Model i utilitzar l'API



# Posada en marxa del fitxer IA.py

















# ü§ñ Exemple b√†sic d'√∫s de BlenderBot amb Hugging Face Transformers

Aquest exemple mostra com utilitzar el model [`facebook/blenderbot-400M-distill`](https://huggingface.co/facebook/blenderbot-400M-distill) per generar respostes a preguntes o missatges de l'usuari, utilitzant el `pipeline` de Hugging Face.

## üì¶ Requisits

Assegura't de tenir instal¬∑lades les depend√®ncies seg√ºents:

```bash
pip install transformers torch
```

## üìú Codi complet

```python
from transformers import pipeline

# Inicialitzem el pipeline amb BlenderBot
# https://huggingface.co/facebook/blenderbot-400M-distill
chatbot = pipeline(task="text2text-generation", model="facebook/blenderbot-400M-distill")

# Missatge de l'usuari
user_message = """
What are some fun activities I can do in the winter?
"""

# Enviem el missatge al model i generem resposta
output = chatbot(user_message)

# Mostrem la resposta generada
print("Bot:", output[0]["generated_text"])
```

## üîç Explicaci√≥ del funcionament

### 1. `chatbot = pipeline(...)`

Aquesta l√≠nia crea un objecte de tipus `Text2TextGenerationPipeline`, especialitzat en tasques de generaci√≥ de text a partir de text (`text2text-generation`).

El model que s'utilitza √©s `facebook/blenderbot-400M-distill`, un model preentrenat per mantenir converses.

### 2. Qu√® √©s `chatbot`

`chatbot` √©s un objecte que es pot cridar com una funci√≥ perqu√® implementa el m√®tode especial `__call__()`.

Gr√†cies a aix√≤, pots fer:

```python
output = chatbot(user_message)
```

### 3. Qu√® fa `chatbot(user_message)`?

- Tokenitza el text d'entrada.
- El passa pel model preentrenat.
- Des-tokenitza la sortida i la retorna en una llista de diccionaris, on cada diccionari cont√© la clau `"generated_text"` amb la resposta generada.

**Exemple de sortida:**

```python
[{'generated_text': 'You can go skiing, ice skating, or drink hot chocolate by the fire.'}]
```

### 4. `print("Bot:", output[0]["generated_text"])`

Accedeix a la primera (i √∫nica) resposta generada i imprimeix el contingut textual.

## ‚úÖ Resultat esperat

```
Bot: You can go skiing, ice skating, or drink hot chocolate by the fire.
```
